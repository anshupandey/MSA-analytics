{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/MSA-analytics/blob/main/Model_Monitoring/Lab5_Model_Monitoring_Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa4f94d6",
      "metadata": {
        "id": "aa4f94d6"
      },
      "source": [
        "# Lab 5: Model Monitoring Dashboard Development\n",
        "**Objective**: Create a dashboard to monitor KPIs and detect anomalies in model behavior.\n",
        "\n",
        "This lab focuses on building a simple real-time dashboard for monitoring Accuracy, Precision, Recall, F1-Score, and PSI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220fdacb",
      "metadata": {
        "id": "220fdacb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/anshupandey/MSA-analytics/refs/heads/main/datasets/Ocean_Hull_Insurance_datasetv2.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa25a954",
      "metadata": {
        "id": "aa25a954"
      },
      "source": [
        "## Simulate Predictions and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2181f377",
      "metadata": {
        "id": "2181f377"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "X = df.drop('Claim_Occurred', axis=1)\n",
        "y = df['Claim_Occurred']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "metrics_dict = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred),\n",
        "    'Precision': precision_score(y_test, y_pred),\n",
        "    'Recall': recall_score(y_test, y_pred),\n",
        "    'F1-Score': f1_score(y_test, y_pred)\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "metrics_df = pd.DataFrame.from_dict(metrics_dict, orient='index', columns=['Value'])\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14dea01",
      "metadata": {
        "id": "c14dea01"
      },
      "source": [
        "## Compute PSI for Monitoring Feature Drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25060ed8",
      "metadata": {
        "id": "25060ed8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_psi(expected, actual, buckets=10):\n",
        "    def scale_range(series, buckets):\n",
        "        return pd.qcut(series.rank(method='first'), buckets, labels=False, duplicates='drop')\n",
        "\n",
        "    if expected.dtype.name == 'category' or expected.dtype == 'object':\n",
        "        expected_dist = expected.value_counts(normalize=True)\n",
        "        actual_dist = actual.value_counts(normalize=True)\n",
        "        all_categories = set(expected_dist.index).union(actual_dist.index)\n",
        "        psi_val = 0\n",
        "        for cat in all_categories:\n",
        "            e_perc = expected_dist.get(cat, 0.0001)\n",
        "            a_perc = actual_dist.get(cat, 0.0001)\n",
        "            psi_val += (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
        "    else:\n",
        "        expected_bins = scale_range(expected, buckets)\n",
        "        actual_bins = scale_range(actual, buckets)\n",
        "        expected_perc = pd.Series(expected_bins).value_counts(normalize=True)\n",
        "        actual_perc = pd.Series(actual_bins).value_counts(normalize=True)\n",
        "        psi_val = 0\n",
        "        for b in range(buckets):\n",
        "            e_perc = expected_perc.get(b, 0.0001)\n",
        "            a_perc = actual_perc.get(b, 0.0001)\n",
        "            psi_val += (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
        "    return psi_val\n",
        "\n",
        "psi_results = []\n",
        "split_index = int(len(df) * 0.7)\n",
        "df_train = df.iloc[:split_index]\n",
        "df_current = df.iloc[split_index:]\n",
        "\n",
        "for col in df.columns:\n",
        "    if col == 'Claim_Occurred':\n",
        "        continue\n",
        "    psi_val = calculate_psi(df_train[col], df_current[col])\n",
        "    psi_results.append({'Feature': col, 'PSI': psi_val})\n",
        "\n",
        "psi_df = pd.DataFrame(psi_results).sort_values(by='PSI', ascending=False)\n",
        "psi_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc96b848",
      "metadata": {
        "id": "dc96b848"
      },
      "source": [
        "## Simulate Dashboard Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d2d8e4",
      "metadata": {
        "id": "90d2d8e4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot metric scores\n",
        "metrics_df.plot(kind='barh', legend=False)\n",
        "plt.title(\"Model Evaluation Metrics\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot PSI values\n",
        "psi_df.set_index('Feature').plot(kind='barh', legend=False)\n",
        "plt.title(\"Feature Stability via PSI\")\n",
        "plt.xlabel(\"PSI Value\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8fcac5a",
      "metadata": {
        "id": "c8fcac5a"
      },
      "source": [
        "## Simulated Alerts for KPI Deviations\n",
        "Based on PSI > 0.2 or F1-Score < 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d3e2e6",
      "metadata": {
        "id": "58d3e2e6"
      },
      "outputs": [],
      "source": [
        "alerts = []\n",
        "\n",
        "if metrics_dict['F1-Score'] < 0.7:\n",
        "    alerts.append(\"⚠️ F1-Score below threshold!\")\n",
        "\n",
        "for _, row in psi_df.iterrows():\n",
        "    if row['PSI'] > 0.2:\n",
        "        alerts.append(f\"⚠️ High PSI detected in {row['Feature']} (PSI = {row['PSI']:.3f})\")\n",
        "\n",
        "alerts if alerts else [\"✅ All metrics are within acceptable range.\"]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}